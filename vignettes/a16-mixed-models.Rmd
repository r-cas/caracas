---
title: "16 - Mixed models with `caracas`"
author: Mikkel Meyer Andersen and Søren Højsgaard
date: "`r date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{16 - Mixed models with `caracas`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  )
options("digits"=4)
```

# Introduction

```{r, message=FALSE, echo=FALSE}
library(caracas)
##packageVersion("caracas")
```

```{r, echo=FALSE}
if (!has_sympy()) {
  # SymPy not available, so the chunks shall not be evaluated
  knitr::opts_chunk$set(eval = FALSE)
  
  inline_code <- function(x) {
    deparse(substitute(x))
  }
}
```

This vignette is based on `caracas` version 
`r packageVersion("caracas")`. `caracas` is avavailable on CRAN at
[https://cran.r-project.org/package=caracas] and on github at 
[https://github.com/r-cas/caracas].

# Linear mixed model

A linear mixed model can be written in general form as

$$
y = Xb + Zu + e
$$


Here, $y$ is a vector of observables, $X$ is a model matrix and $b$ is a corresponding vector of
regression coefficients. Also $Z$ is a model matrix and $u$ is a
corresponding vector of random effects. Lastly, $e$ is also a vector
of random errors. Because there are two random effects ($u$ and $e$)
such models are often called mixed models. qIt is assumed that $u$ and $e$ are independent and that $u \sim N(0,
G)$ and $e \sim N(0, R)$. Typically $G$ and $R$ depend on unknown parameters $\theta$, so we may write $G(\theta)$ and $R(\theta)$ instead.
 
Consequently 

$$
E(y)=\mu=Xb, \quad Var(y) = V(\theta) =ZG(\theta)Z'+R(\theta).
$$


The log-likelihood is

$$
logL(b, \theta) = -\frac 1 2 (\log|V(\theta)| + (y-Xb)'V(\theta)^{-1}(y-Xb)).
$$

For any value of $\theta$, the MLE for $b$ is

$$
\hat b =\hat b(\theta) = (X'V(\theta)^{-1}X)^{-1}X'V(\theta)^{-1}y.
$$

Plugging this estimate into the log-likelihood gives the profile log-likelihood which is a function of $\theta$ only:

$$
plogL(\theta) = - \frac 1 2 (\log|V(\theta)| + (y-X\hat b(\theta))'V(\theta)^{-1}(y-X\hat b(\theta))).
$$


We illustrate fitting a mixed model based on a subset of the shoes
data available, e.g. in the MASS and doBy packages. We also compare
the results with the output from lmer() if the lme4 package is
installed. All caracas symbols are postfixed with an underscore.

```{r}
shoes_long <- data.frame(
  stringsAsFactors = FALSE,
  type = c("A", "B", "A", "B", "A", "B", "A", "B"),
  wear = c(13.2, 14, 8.2, 8.8, 10.9, 11.2, 14.3, 14.2),
  boy = as.factor(c("1", "1", "2", "2", "3", "3", "4", "4")))

y <- shoes_long$wear
X <- model.matrix(~type, data=shoes_long) |> head(10)
Z <- model.matrix(~-1+boy, data=shoes_long) |> head(10)
y |> head()
X |> head()
Z |> head()
```


# Fitting model with `caracas`

We define the following symbols in caracas:

```{r}
y_ <- as_sym(y)
X_ <- as_sym(X)
Z_ <- as_sym(Z)
b_ <- vector_sym(2, "b")
def_sym(tau2, sigma2)
```

```{r}
## Covariance matrices for random effects
G_  <- diag_("tau^2", ncol(Z))
R_  <- diag_("sigma^2", nrow(Z))
```

So for this example, $\theta=(\tau, \sigma)$. The variance of $y$ is

```{r}
## Variance etc of y
V_  <- Z_ %*% G_ %*% t(Z_) + R_
Vi_ <- solve(V_)
detV_ <- determinant(V_, log=FALSE)
```


$$
`r tex_list("G=", G_, ", R=", R_, zero_as_dot=T)`
$$

$$
`r tex_list("V=", V_, zero_as_dot=T)`
$$


A programmatic approach is to define a function returning the log-likelihood and the profile log-likelihood as a caracas symbol. Notice that we supply the both $V$, $V^{-1}$ and $\log|V|$ as arguments to the log-likelihood function. There is redundancy in this, but it is convenient to be able to supply alternative forms for $V$. 

```{r}
get_logL <- function(y, X, b, V, Vi=solve(V), detV=determinant(V, log=FALSE)) {
    res <- function(y, X, b) {
        return(y - X %*% b)
    }
    
    qform <- function(res, Vi) {
        return(t(res) %*% Vi %*% res)
    }
    res <- res(y, X, b)
    Q   <- qform(res, Vi)
    return(-0.5 * (log(detV) + Q))
}

get_bhat <- function(X, y, Vi) {
    return(solve(t(X) %*% Vi %*% X, t(X) %*% Vi) %*% y)
}

get_V <- function(Z, G, R) {
    return(Z %*% G %*% t(Z) + R)
}

get_J <- function(logL, b) {
    return(-der2(logL, b))
}
```

```{r}



handle_order <- function(e, order) {
    if (is.null(order))
        return(sort(all.vars(e)))
    
    nms <- all.vars(e)
    ss <- setdiff(nms, order)
    if (length(ss) > 0) {        
        stop("some arguments are not given in 'order'")
    }
    return(order)
}
  

expr_to_string <- function(ee) {
    ee_str <- lapply(ee, deparse)
    
    ee_str <-
        lapply(ee_str,
               function(e_) {
                   paste0(e_, collapse="\n")               
               })
    ee_str
}

expr_to_multi_param_fun <- function(ee, order=NULL) {

    nms <- handle_order(ee, order)   
    ee_str <- expr_to_string(ee)
    
    fun_str <- paste0("function(", paste0(nms, collapse=", "), ")")
        
    bd <- paste0("\n{ \n", paste0(ee_str, collapse=";\n "), "\n}")
    ff <- paste0(fun_str, bd)
    fun <- eval(parse(text=ff))
    return(fun)
}


```


```{r, eval=FALSE}
V_    <- get_V(Z_, G_, R_)
V_fn <- as_func(V_, vec_arg = TRUE)

get_bhat <- function(X, y, Vi) {
    return(solve(t(X) %*% Vi %*% X, t(X) %*% Vi) %*% y)
}
get_bhat(X_, y_, V_)

X. <- as_expr(X_)
y. <- y

ee <- expression({
  V. <- V_fn(parm)
  detV. <- det(V.)
  Vi. <- solve(V.)
  b. <- (solve(t(X.) %*% Vi. %*% X., t(X.) %*% Vi.) %*% y.)
  r. <- y - X. %*% b.
  Q. <- (t(r.) %*% Vi. %*% r.)
  logL. <- -0.5 * (log(detV.) + Q.)
  as.numeric(logL.)
})

solve(t(X) %*% Vi %*% X, t(X) %*% Vi) %*% y

ee_str <- expr_to_string(ee)

nms <- c("parm", "X.", "y.", "V_fn")
fun_str <- paste0("function(", paste0(nms, collapse=", "), ")")
fun_str
    
bd <- paste0("\n{ \n", paste0(ee_str, collapse=";\n "), "\n}")
ff <- paste0(fun_str, bd)
fun <- eval(parse(text=ff))
fun
    
    out <- optim(c(.1, .1), fun, X.=X., y.=y., V_fn=V_fn, method="L-BFGS-B", 
#          lower=c(1e-6, 1e-6), upper=c(Inf, Inf),
              control=list(fnscale=-1), hessian=TRUE)

out$par    
V. <- V_fn(out$par)    
Vi. <- solve(V.)
b. <- (solve(t(X.) %*% Vi. %*% X., t(X.) %*% Vi.) %*% y.)
b.    
```




## Maximizing the profile likelihood

```{r}
V_    <- get_V(Z_, G_, R_)
detV_ <- determinant(V_, log=FALSE)
Vi_   <- solve(V_)
bhat_ <- get_bhat(X_, y_, Vi_) 

plogL_ <- get_logL(y_, X_, bhat_, V_, Vi_, detV_) |> simplify()
logL_  <- get_logL(y_, X_, b_,    V_, Vi_, detV_) |> simplify()



## Or
## plogL_ <- get_logL(y_, X_, bhat_, V_) |> simplify()
## logL_  <- get_logL(y_, X_, b_,    V_) |> simplify()

J_ <- get_J(logL_, b_) 

out <- optim_sym(c(.1, .1), plogL_, method="L-BFGS-B", 
              control=list(fnscale=-1), hessian=TRUE)
var_par <- out$par

bhat_ <- subs(bhat_, as.list(var_par))
J_    <- subs(J_, as.list(var_par))
Vb_   <- solve(J_)

as_expr(bhat_)
as_expr(Vb_)
```



## Maximizing the full likelihood

To maximize the full likelihood, we need to maximize with respect to
both the variance parameters and the fixed effects. For this to work
we often need to impose restrictions on the parameter space (not
necessary for this specific example)


```{r}
out <- optim_sym(c(0, 0, .1, .1), logL_, method="L-BFGS-B", 
          lower=c(-Inf, -Inf, 1e-6, 1e-6), upper=c(Inf, Inf, Inf, Inf),
              control=list(fnscale=-1), hessian=TRUE)
out$par
solve(-out$hessian)[1:2, 1:2] |> round(4)
## This also works - but is fragile
## optim_sym(c(0, 0, .1, .1), logL_, method="L-BFGS-B", 
##               control=list(fnscale=-1), hessian=TRUE)
```


## Reparametrizing

It is often useful to reparametrize the model to avoid constraints on
the parameters. Three typical cases are:

1. If $\alpha$ must be positive (e.g. a variance), reparametrize
   $\alpha = \exp(w)$ and optimize with respect to $w$ which is
   unconstrained.

2. If $\alpha$ must be in $[0, 1]$, (e.g. a probability) reparametrize
   $\alpha = \frac{\exp(w)}{1+\exp(w)}$ and optimize with respect to
   $w$ which is unconstrained.

3. If $\alpha$ must be in $[-1, 1]$, (e.g. a correlation)
   reparametrize $\alpha = \frac{\exp(w)-1}{\exp(w)+1} = \tanh(w)$ and
   optimize with respect to $w$ which is unconstrained.

```{r}
V2_ <- subs(V_, list(sigma="exp(log_sigma)", tau="exp(log_tau)")) |> simplify()
detV2_ <- determinant(V2_, log=FALSE)
V2i_ <- solve(V2_)
bhat2_ <- get_bhat(X_, y_, V2i_)

plogL2_ <- get_logL(y_, X_, bhat_, V2_, V2i_, detV2_) |> simplify()
logL2_  <- get_logL(y_, X_, b_,    V2_, V2i_, detV2_) |> simplify()

## Or
## plogL2_ <- get_logL(y_, X_, bhat_, V2_) |> simplify()
## logL2_  <- get_logL(y_, X_, b_,    V2_) |> simplify()

out <- optim_sym(c(.1, .1), plogL2_, method="L-BFGS-B", 
              control=list(fnscale=-1), hessian=TRUE)

var_par <- out$par
var_par |> exp()

out <- optim_sym(c(0, 0, .1, .1), logL2_, method="L-BFGS-B", 
              control=list(fnscale=-1), hessian=TRUE)

var_par <- out$par
solve(-out$hessian)[1:2, 1:2]
```

```{r, eval=F}
def_sym(sigma)
B <- as_sym(toeplitz(c(1, "rho")))
V3_ <- sigma^2 * kronecker(diag_(1, ncol(Z)), B)
V3_

V3i_ <- solve(V3_)
bhat3_ <- get_bhat(X_, y_, V3i_)

plogL3_ <- get_logL(y_, X_, bhat_, V3_) |> simplify()
logL3_  <- get_logL(y_, X_, b_,    V3_) |> simplify()

out <- optim_sym(c(.1, .1), plogL3_, method="L-BFGS-B", 
             lower=c(-1+1e-6, 1e-6), upper=c(1-1e-6, Inf),
             control=list(fnscale=-1), hessian=TRUE)

var_par <- out$par
var_par

eps <- 1e-6
out <- optim_sym(c(0, 0, .1, .1), logL3_, method="L-BFGS-B", 
          lower=c(-Inf, -Inf, -1+eps, eps), upper=c(Inf, Inf, 1-eps, Inf),
              control=list(fnscale=-1), hessian=TRUE)

var_par <- out$par
var_par
solve(-out$hessian)[1:2, 1:2]

V3_ <- subs(V3_, list(rho="(exp(2*w)-1)/(exp(2*w)+1)"))
logL3_  <- get_logL(y_, X_, b_,    V3_) |> simplify()

out <- optim_sym(c(0, 0, .1, .1), logL3_, method="L-BFGS-B", 
#          lower=c(-Inf, -Inf, -1+eps, eps), upper=c(Inf, Inf, 1-eps, Inf),
              control=list(fnscale=-1), hessian=TRUE)

var_par <- out$par
var_par
tanhinv <- function(z) {
    return(log((1+z)/(1-z))/2)
}
tanh(var_par[4])
```

```{r}
library(nlme)
gls(wear ~ type, correlation = corCompSymm(form = ~ 1 | boy), 
    method="ML",
    data = shoes_long)

```



# Comparison with  with lmer() - if available

```{r}
var_par
as.numeric(as_expr(bhat_))
as_expr(Vb_)

if (require(lme4)){
    lmm_fit  <- lmer(wear ~ type + (1|boy), data=shoes_long, REML=FALSE)
    ## X <- getME(lmm_fit, "X") 
    ## Z <- getME(lmm_fit, "Z") 
    print(VarCorr(lmm_fit))
    print(fixef(lmm_fit))
    print(vcov(lmm_fit))
}
```




```{r, echo=FALSE, eval=FALSE}
## OBSOLETE NOW
## For known value of variance Var(y), the MLE for b is
b_hat_ <- solve(t(X_) %*% Vi_ %*% X_, t(X_) %*% Vi_) %*% y_
b_hat_ <- b_hat_ |> simplify()
b_hat_

## The profile likelihood
res0_  <- (y_ - X_ %*% b_hat_) |> simplify()
Q0_    <- (t(res0_) %*% Vi_ %*% res0_) |> simplify()
plogL_ <- -0.5 * (log(detV_) + Q0_)
plogL_func <- as_func(plogL_, vec_arg = T)

## Now optimize
fit <- optim(c(.1, .1), plogL_func, method="L-BFGS-B", 
              control=list(fnscale=-1), hessian=TRUE)
var_par <- fit$par
names(var_par) <- eval(formals(plogL_func)$names_parm) # 2.27, 0.24
var_par 
sqrt(var_par) ## Same as lmer

## Substitute variance parameters into expression for b
b_hat <- subs(b_hat_, as.list(var_par))
b_hat ## 11.64 0.04

## The full likelihood
res1_ <- y_ - X_ %*% b_
Q1_ <- (t(res1_) %*% Vi_ %*% res1_) |> simplify()
Q1_
logL_ <- -0.5 * (log(detV_) + Q1_)
logL_func <- as_func(logL_, vec_arg = T)

H_ <- der2(logL_, b_) 
H_  <- subs(H_, as.list(var_par))
J_  <- solve(-H_)
as_expr(J_) 
```




```{r, eval=F, echo=F}
get_logL <- function(y.c, X.c, b.c, V.c, Vi.c=solve(V.c), detV.c=determinant(V.c, log=FALSE)) {
    res <- function(y.c, X.c, b.c) {
        return(y.c - X.c %*% b.c)
    }
    
    qform <- function(res.c, Vi.c) {
        return(t(res.c) %*% Vi.c %*% res.c)
    }
    res.c <- res(y.c, X.c, b.c)
    Q.c   <- qform(res.c, Vi.c)
    return(-0.5 * (log(detV.c) + Q.c))
}


get_bhat <- function(X.c, y.c, Vi.c) {
    return(solve(t(X.c) %*% Vi.c %*% X.c, t(X.c) %*% Vi.c) %*% y.c)
}

get_V <- function(Z.c, G.c, R.c) {
    return(Z.c %*% G.c %*% t(Z.c) + R.c)
}

get_J <- function(logL.c, b.c) {
    return(-der2(logL.c, b.c))
}
```
